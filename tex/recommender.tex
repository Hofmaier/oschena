\documentclass[a4paper, 11pt]{article}
\usepackage[german]{babel}
\usepackage{amsmath}
\usepackage{pgfplots}
\author{Lukas Hofmaier}
\title{Recommender}
\begin{document}
\maketitle
\section{User-user collaborative filtering}
User base filtering sucht nach user, die "ahnlich sind. 
Die Technik um "ahnliche User zu finden heisst kNN (k neirest neighbors).
\subsection{Similarity}
Um zwei User miteinander zu vergleichen, kann die Pearson Similarity eingesetzt werden.
Was passiert mit User die keine gemeinschaftlichen Item gerated haben? 
Wenn es keinen User gibt, der das selbe Item bewertet hat, ist der Korrelationskoeffizient -1.
Die Pearson Similarity ber"ucksichtigt nur gemeinsam geratete Daten.
Was passiert wenn zwei User ein einziges gemeinsames Item haben und dieses gleich bewerten?

F"ur die Distanz oder die "Ahnlichkeit gibt es verschiedene Metriken. 
\begin{description}
\item[Euklidische Distanze] Jede Wertung entspricht dem Wert einer Dimension. Die euklidische Distanz berechnet einfach den geometischen Abstand.
\item[Pearson Similarity] Die Pearson Similariy ber"ucksichtig User die Items konstant tiefer bewerten. Wenn man zwei User die mit den Vektoren (1,2,3,4) und (2,3,4,5) dargestellt werden erhalten die maximale "Ahnlichkeit von 1.
\end{description}

\begin{equation}
 sim(u,v) = \frac{cov(X,Y}{\sigma_X \sigma_Y)} = \frac{(r_{u,i} - \bar{r}_u)(r_{v,i} - \bar{r}_v)}{r_{v,i} - \bar{r}_v}    
\end{equation}
Dann zeigt der Pearson Correlation Coffezient eine hohe Aehnlichkeit an.

\subsection{Computing Prediction}
\label{sec:compp}

M"ochte man f"ur einen User f"ur ein bestimmtes Item ein Rating vorhersagen, kann man folgendermassen vorgehen.

\begin{enumerate}
\item neigborhood bestimmen
\item F"ur alle User in der neighborhood, die das Item bewertet haben, multipliziert man das Rating des entsprechenden Users mit dem Wert der Similarity. Dadurch werden Ratings von User, die sehr "ahnlich sind st"arker gewichtet. Man berechnet den Durchschnitt dieser Ratings.

\end{enumerate}

\begin{equation}
  \label{eq:computeprediction}
  p_{u,i} = \frac{\sum_{u' \in N}{s(u,u') r_{u',i}}}{\sum_{u' \in N}{|s(u,u')|}}
\end{equation}

Dabei wird noch nicht ber"ucksicht, dass bestimmte User permanent h"ohere Wertungen geben.

\section{Matrix Factorization}
\label{sec:matrixfactorization}

Bei Matrixfactorization geht es darum, dass man eine R, die d"unn besetzt ist, voll ausf"ullt. Man geht davon aus Users und Items sogenannte Latent Factors haben. Das heisst ein User hat zum Beispiel bestimmte Preferenzen. Er mag Action Filme aber keine Romanzen. Ein Film kann ebenfalls nach den selben Dimensionen bewertet werden. Das Rating ist das Skalarprodukt der Preferenzen des Users und den Eigenschaften des Items.

\begin{equation}
  \label{eq:latentfactors}
  R_{ui} = q_i^T p_u
\end{equation}

Da man die Latent factors nicht hat, m"ochte man sie approximieren. Die Residuen kann man berechnen indem man, die berechneten Werte von den vorhandenen Werten in R subtrahiert. Die Summe aller Residuen soll minimiert werden \cite{koren2009}`.
\begin{equation}
  \label{eq:optimization}
    R_{ui} - q_i^T p_u + \lambda (\lVert q \rVert^2 + \lVert p \lVert ^2)
\end{equation}

Damit die Elemente der Latent Factor Vectors nicht zu gross werden, werden sie reguliert.
 
Um das Minimum zu finden kann beispielsweise das Verfahren Gradientenabstieg angewendet werden. Dazu wird die Kostenfunktion nach $q_i$ und nach $p_u$ abgeleitet. F"ur $q_i$ lautet die Ableitung:
\begin{equation}
  \label{eq:decx}
  \frac{ \partial }{ \partial q_i } = \sum (q_i^T p_i - r) q_i + \lambda p_i
\end{equation}

F"ur die $p_i$ wird folgendermassen abgeleitet.

\section{Daten}
\label{sec:data}

F"ur das Projekt wurden die Daten von Movielens verwendet. Daten in u.data sind in Kolonnen strukturiert. Erste Kolonne user id, zweite item.id, dritte rating, vierte timestamp. 

u1.base hat 80000 ratings. 943 User.

Movielens data set hat 5 training test paare. u1.base - u1.test. 
training/test ration = x = 80000/20000

\section{Arbeitsspeicher Nutzung}
\label{sec:ram}

Bei einer Ratingmatrix r mit der Dimension von 1682 mal 943 konsumiert das Programm zu viel Arbeitsspeicher. Es werden 10GB angefordert. Profiling hat gezeigt die Fehler Berechnung viel Arbeitsspeicher konsumiert.

\begin{equation}
  \label{eq:squareerror}
  \sum_{(u,i) \in \kappa} (r_{ui} - q_i^T p_u)^2
\end{equation}

$\kappa$ ist die Menge aller User-Item Paare, f"ur die ein Rating bekannt ist.

\section{Evaluation}
\label{sec:evaluation}

Um die Recommender Techniken zu evaluieren wurden sie auf ein bestehendes Testset angewendet. Dabei wird der Mean Absolte Error gemessen.

Der MAE ist wie folgt definiert \cite{sarwar01}:
\begin{equation}
  \label{eq:mae}
  MAE = \frac{p_i-q_i}{N}
\end{equation}

\begin{tikzpicture}
  \begin{axis}
    title = Mean Absolute Error,
    xlabel = Technik,
    ylabel = MAE,
  \end{axis}
\end{tikzpicture}

Es werden ein Baselinealgorithmus, User-user und Matrixfactorization miteinander verglichen.

\bibliographystyle{plain}
\bibliography{a}
\end{document}